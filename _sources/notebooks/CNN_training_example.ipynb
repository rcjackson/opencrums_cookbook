{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5e6435-fc49-4873-8313-0208b4daa4c4",
   "metadata": {},
   "source": [
    "### Code for training Convolutional Neural Network\n",
    "\n",
    "First, load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d997b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Add, ReLU, Conv2DTranspose, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bab506-7b59-4e03-bcb4-34c5e7c16cbe",
   "metadata": {},
   "source": [
    "Next, we load the EPA AirNow data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda44276",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_now_data = glob.glob('/lcrc/group/earthscience/rjackson/epa_air_now/*.csv')\n",
    "air_now_df = pd.concat(map(pd.read_csv, air_now_data))\n",
    "air_now_df['datetime'] = pd.to_datetime(air_now_df['DateObserved'] + ' 00:00:00')\n",
    "air_now_df = air_now_df.set_index('datetime')\n",
    "air_now_df = air_now_df.sort_index()\n",
    "print(air_now_df['CategoryNumber'].values.min())\n",
    "\n",
    "# How many timesteps ahead should we predict?\n",
    "lag = 0\n",
    "site = \"hou\"\n",
    "\n",
    "# Insert the path to your cropped dataset here\n",
    "input_merra_path = '/lcrc/group/earthscience/rjackson/MERRA2/hou_reduced/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f310c",
   "metadata": {},
   "source": [
    "This next block of code will grab the EPA AirNow classification given a particular datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d87519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_air_now_label(time):\n",
    "    if np.min(np.abs((air_now_df.index - time))) > timedelta(days=1):\n",
    "        return np.nan\n",
    "    ind = np.argmin(np.abs(air_now_df.index - time))\n",
    "    return air_now_df['CategoryNumber'].values[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487cd07-60a8-47ec-ba64-1b41e1185116",
   "metadata": {},
   "source": [
    "This next block of code loads the cropped MERRA2 dataset. \n",
    "\n",
    "load_data takes in a list of aerosol species as inputs. The aerosol species are the abbreviations used in the MERRA2 dataset (i.e. BC, SO4, SO2, DU, SS, OC, etc.). In addition, the *test_data_size* parameter specifies the fraction of the dataset to be set aside as a testing dataset for training.\n",
    "\n",
    "The returned parameters from load_data are:\n",
    "\n",
    "   * x_dataset_train, x_dataset_test: Dictionaries containing the test and training dataset inputs\n",
    "   * y_train, y_test: The training and test labels corresponding to x_dataset\n",
    "   * shape: The shape of the input x data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f060af-adf4-4dbe-b3c4-e83e998ffdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(species, test_data_size=0.20):\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sCMASS*.nc' % species).sortby('time')\n",
    "    print(ds)\n",
    "    times = np.array(list(map(pd.to_datetime, ds.time.values)))\n",
    "    x = ds[\"%sCMASS\" % species].values\n",
    "    old_shape = x.shape\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = scaler.transform(\n",
    "            np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = np.reshape(x, old_shape)\n",
    "    inputs = np.zeros((old_shape[0], old_shape[1], old_shape[2], 3))\n",
    "    inputs[:, :, :, 0] = x\n",
    "    ds.close()\n",
    "    if species == \"SO4\" or species == \"DMS\" or species == \"SO2\":\n",
    "       inp = \"SU\"\n",
    "    else:\n",
    "       inp = species\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sFLUXU*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXU\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 1] = x2\n",
    "    ds.close()\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sFLUXV*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXV\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 2] = x2\n",
    "    classification = np.array(list(map(get_air_now_label, times)))\n",
    "    where_valid = np.isfinite(classification)\n",
    "    inputs = inputs[where_valid, :, :, :]\n",
    "    classification = classification[where_valid]\n",
    "    # Use classification from x days ahead\n",
    "    if lag > 0:\n",
    "        classification = classification[lag*8:]\n",
    "        inputs = inputs[:lag*8, :, : :]\n",
    "\n",
    "    y = tf.one_hot(classification, 5).numpy()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "            inputs, y, test_size=test_data_size)\n",
    "    shape = inputs.shape\n",
    "    x_dataset_train = {'input_1': np.squeeze(x_train[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_train[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_train[:, :, :, 2])}\n",
    "    x_dataset_test = {'input_1': np.squeeze(x_test[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_test[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_test[:, :, :, 2])}\n",
    "\n",
    "    return x_dataset_train, x_dataset_test, y_train, y_test, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dea24",
   "metadata": {},
   "source": [
    "Construct the parallel CNN model. This procedure takes in 3 parameters:\n",
    "\n",
    "   * shape: the shape of the input data as a tuple\n",
    "   * the_dict: A dictionary containing the model hyperparameters\n",
    "   * dataset: The input dataset returned by *load_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edbb74-3ef0-4cb4-8c0d-6d718e91a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(shape, the_dict, dataset):\n",
    "    width = shape[2]\n",
    "    height = shape[1]\n",
    "    mpool_1s = []\n",
    "    in_layers = []\n",
    "    dict_keys = list(dataset.keys())\n",
    "    for j in range(len(dict_keys)):\n",
    "        inp_layer = Input(shape=(height, width, 1), name=dict_keys[j])\n",
    "        mpool_1 = inp_layer\n",
    "        in_layers.append(inp_layer)\n",
    "        for i in range(the_dict['num_layers']):\n",
    "            conv2d_1 = Conv2D(the_dict['num_channels'],\n",
    "                (2, 2), activation=the_dict['activation'], padding='same',\n",
    "                kernel_initializer='he_normal')(mpool_1)\n",
    "            conv2d_1 = BatchNormalization()(conv2d_1)\n",
    "            mpool_1 = MaxPooling2D((2, 2))(conv2d_1)\n",
    "        mpool_1s.append(Flatten()(mpool_1))\n",
    "    flat_1 = Add()(mpool_1s)\n",
    "    \n",
    "    for i in range(the_dict['num_dense_layers']):\n",
    "        flat_1 = Dense(the_dict['num_dense_nodes'], activation='relu'\n",
    "                )(flat_1)\n",
    "        flat_1 = BatchNormalization()(flat_1)\n",
    "\n",
    "    output = Dense(5, activation=\"softmax\", name=\"class\")(flat_1)\n",
    "    \n",
    "    return Model(in_layers, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fae491-3fd3-47f5-8202-caf43ead4ade",
   "metadata": {},
   "source": [
    "Do the model training and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict):\n",
    "    x_ds_train = {}\n",
    "    x_ds_test = {}\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    # Adjust to the species you want to have\n",
    "    species_list = config['species_list']\n",
    "    \n",
    "    for species in species_list:\n",
    "        print(species)\n",
    "        x_ds_train1, x_ds_test1, y_train, y_test, shape = load_data(species)\n",
    "        x_ds_train.update(x_ds_train1)\n",
    "        x_ds_test.update(x_ds_test1)\n",
    "    \n",
    "    model = classifier_model(shape, config, x_ds_test)\n",
    "    model.compile(optimizer=Adam(lr=config[\"learning_rate\"]),\n",
    "        loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    # AQI classes inbalanced, need weights\n",
    "    class_weight = {0: 1, 1: 2, 2: 20, 3: 90, 4: 510}\n",
    "    history = model.fit(\n",
    "            x_ds_train, y_train, \n",
    "            validation_data=(x_ds_test, y_test), epochs=config[\"num_epochs\"],\n",
    "            batch_size=config[\"batch_size\"], class_weight=class_weight)\n",
    "    model.save('../models/classifier-%s-lag-%d' % (site, lag))\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ee918-a833-4781-ba40-392f5b5caf48",
   "metadata": {},
   "source": [
    "Call the model training code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915325d9-784d-49a0-851c-93fe664d553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "        \"num_epochs\": 50,        # Number of training epochs. Recommended to use 200+\n",
    "        \"num_channels\": 128,     # Number of channels in Conv2D layer \n",
    "        \"learning_rate\": 0.0001, # Learning rate for training\n",
    "        \"num_dense_nodes\": 8,    # Number of nodes in each dense layer of the classifier\n",
    "        \"num_dense_layers\": 3,   # Number of layers in classifier portion\n",
    "        \"activation\": \"relu\",    # Activation function used (ReLU is typically good)\n",
    "        \"batch_size\": 10,        # Batch size for training\n",
    "        \"num_layers\": 2,         # Number of Conv2D layers in feature extractor\n",
    "        # Aerosol species to use in model training\n",
    "        \"species_list\": ['SS', 'SO4', 'SO2', 'OC', 'DU', 'DMS', 'BC']}\n",
    "\n",
    "history = run(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597658e7-59cc-4a86-ba84-a168ab75af48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
