{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed9d5b5-6ee2-4962-9bd2-e9a6b6ab9afa",
   "metadata": {},
   "source": [
    "### Combining it all together\n",
    "\n",
    "This cookbook shows how to visualize the Layerwise Relevance Propogation data that was generated to determine the importance of given features to the AQI classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737a087-a6ba-4016-a038-e46854abf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c847d5-d6f7-49ce-9807-438c79ede5e3",
   "metadata": {},
   "source": [
    "Query local directories for all of the input RLP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0428c-d872-4d0c-8e57-e1be1506e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "pickles = '/lcrc/group/earthscience/rjackson/opencrums/scripts/relevance_pickles/'\n",
    "out_plot_path = '/lcrc/group/earthscience/rjackson/merra_relevances/'\n",
    "hour = int(sys.argv[1])\n",
    "pickle_list = glob.glob(pickles + 'rel-%dhr*.pickle' % hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e79640-f2a8-4471-aae9-dd54c5f3f18d",
   "metadata": {},
   "source": [
    "Load a copy of the MERRA2 data to get the base lat/lon grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045a187-3117-4a5a-9b41-49c31b493e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lats, lons for plotting\n",
    "ds = xr.open_mfdataset(\n",
    "            '/lcrc/group/earthscience/rjackson/MERRA2/se_reduced/DUCMASS*.nc')\n",
    "print(ds.time)\n",
    "x = ds[\"DUCMASS\"].values\n",
    "lon = ds[\"lon\"].values\n",
    "lat = ds[\"lat\"].values\n",
    "lon_inds = np.argwhere(\n",
    "            np.logical_and(\n",
    "                ds.lon.values >= ax_extent[0],\n",
    "                ds.lon.values <= ax_extent[1])).astype(int)\n",
    "lat_inds = np.argwhere(\n",
    "            np.logical_and(\n",
    "                ds.lat.values >= ax_extent[2],\n",
    "                ds.lat.values <= ax_extent[3])).astype(int)\n",
    "lon = lon[lon_inds]\n",
    "lat = lat[lat_inds]\n",
    "\n",
    "time = ds[\"time\"].values\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c505db4-1cc0-4446-9736-48e9e34c4272",
   "metadata": {},
   "source": [
    "Calculate the normalized relevance for each time period. The normalized relevance here simply scales each of the input relevances from 0 to 1, where 0 is the minimum value of relevance for the time period and 1 is the maximum relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a4300-6dbc-4f45-a6a9-9ad46bcdec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = ['Good', 'Moderate', 'Unhealthy Sens.', 'Unhealthy', 'Hazardous']\n",
    "mean_relevances = {}\n",
    "p = open(pickle_list[0], mode=\"rb\")\n",
    "relevances = pickle.load(p)\n",
    "input_keys = []\n",
    "for key in relevances.keys():\n",
    "    if \"input_\" in key:\n",
    "        input_keys.append(key)\n",
    "        mean_relevances[key] = np.zeros((5, lon.shape[0], lon.shape[1]))\n",
    "\n",
    "for picks in pickle_list:\n",
    "    p = open(picks, mode='rb')\n",
    "    relevances = pickle.load(p)\n",
    "    classes = relevances['output'].numpy().argmax(axis=1)\n",
    "    aqi = relevances['aqi'].argmax(axis=1)\n",
    "    for k in input_keys:\n",
    "        relevances[k] = relevances[k].numpy()\n",
    "\n",
    "    \n",
    "    for j in range(len(classes)):\n",
    "        for k in input_keys:\n",
    "            r_min = relevances[k][j, :, :].min()\n",
    "            r_max = relevances[k][j, :, :].max()\n",
    "            relevances[k][j, :, :] = (relevances[k][j, :, :] - r_min) / (r_max - r_min)\n",
    "    true_times = np.array([x in pre_trough for x in soms])\n",
    "\n",
    "    soms = np.array([get_som(x) for x in relevances['time']])\n",
    "            \n",
    "    for j in range(len(classes)):\n",
    "        sum_all_r = np.squeeze(np.max(np.concatenate(\n",
    "            [relevances[k][j, :, :] for k in input_keys])))\n",
    "        num_points[classes[j]] = num_points[classes[j]] + 1 \n",
    "        for k in input_keys:\n",
    "            mean_relevances[k][classes[j], :, :] += np.squeeze(\n",
    "                relevances[k][j, :, :]) \n",
    "            \n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a323e-ee46-4c32-a43e-69a330fe60b8",
   "metadata": {},
   "source": [
    "Generate the dataset-wide averages of normalized relevance. Here, relevances near 0 should be interpreted as not relevant, 0.5 as no preference, and 1 as very relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865dab0-d658-4f4b-8982-9b0aef685218",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max = -np.inf\n",
    "r_mean = 0\n",
    "i = 0\n",
    "for j in range(5):\n",
    "    for k in input_keys:\n",
    "        mean_relevances[k][j,:,:] /= num_points[j]\n",
    "        r_max = np.max([r_max, np.percentile(mean_relevances[k][j, :, :], 95)])\n",
    "        r_mean += np.mean(mean_relevances[k][j, :, :])\n",
    "        i += 1\n",
    "r_mean = r_mean / i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce770d24-fecd-4405-98c9-e7e6935daff3",
   "metadata": {},
   "source": [
    "Use Cartopy to generate the plots of relevance over the Houston domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6e4be-3fdd-468f-bedf-5ca701e93c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_provinces = cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none')\n",
    "\n",
    "for key in input_keys:\n",
    "    fig, ax = plt.subplots(5, 1,\n",
    "            subplot_kw=dict(projection=ccrs.PlateCarree()),\n",
    "            figsize=(10, 15))\n",
    "    for l in range(1, 6):     \n",
    "        r = np.squeeze(mean_relevances[key][l - 1])\n",
    "        \n",
    "        print(r.max())\n",
    "        c = ax[l - 1].contourf(lon, lat, r,\n",
    "            cmap='seismic', levels=np.arange(0, 1., 0.01))\n",
    "        bar = plt.colorbar(c, label='Normalized relevance',\n",
    "                ax=ax[l - 1])\n",
    "       \n",
    "        ax[l - 1].coastlines()\n",
    "        ax[l - 1].add_feature(states_provinces)\n",
    "        ax[l - 1].add_feature(cfeature.BORDERS)\n",
    "        ax[l - 1].set_title(classification[l-1])\n",
    "        ax[l - 1].set_xlabel('Latitude')\n",
    "        ax[l - 1].set_ylabel('Longitude')\n",
    "    fig.savefig('output_relevance_/relevance-%s.png' % (key))\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
