{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5e6435-fc49-4873-8313-0208b4daa4c4",
   "metadata": {},
   "source": [
    "### Code for training Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d997b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Add, ReLU, Conv2DTranspose, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda44276",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_now_data = glob.glob('/lcrc/group/earthscience/rjackson/epa_air_now/*.csv')\n",
    "air_now_df = pd.concat(map(pd.read_csv, air_now_data))\n",
    "air_now_df['datetime'] = pd.to_datetime(air_now_df['DateObserved'] + ' 00:00:00')\n",
    "air_now_df = air_now_df.set_index('datetime')\n",
    "air_now_df = air_now_df.sort_index()\n",
    "print(air_now_df['CategoryNumber'].values.min())\n",
    "\n",
    "# How many timesteps ahead should we predict?\n",
    "lag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f310c",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d87519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_air_now_label(time):\n",
    "    if np.min(np.abs((air_now_df.index - time))) > timedelta(days=1):\n",
    "        return np.nan\n",
    "    ind = np.argmin(np.abs(air_now_df.index - time))\n",
    "    return air_now_df['CategoryNumber'].values[ind]\n",
    "\n",
    "\n",
    "def load_data(species):\n",
    "    ds = xr.open_mfdataset('/lcrc/group/earthscience/rjackson/MERRA2/hou_reduced/%sCMASS*.nc' % species).sortby('time')\n",
    "    print(ds)\n",
    "    times = np.array(list(map(pd.to_datetime, ds.time.values)))\n",
    "    x = ds[\"%sCMASS\" % species].values\n",
    "    old_shape = x.shape\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = scaler.transform(\n",
    "            np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = np.reshape(x, old_shape)\n",
    "    inputs = np.zeros((old_shape[0], old_shape[1], old_shape[2], 3))\n",
    "    inputs[:, :, :, 0] = x\n",
    "    ds.close()\n",
    "    if species == \"SO4\" or species == \"DMS\":\n",
    "       inp = \"SU\"\n",
    "    else:\n",
    "       inp = species\n",
    "    ds = xr.open_mfdataset('/lcrc/group/earthscience/rjackson/MERRA2/hou_reduced/%sFLUXU*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXU\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 1] = x2\n",
    "    ds.close()\n",
    "    ds = xr.open_mfdataset('/lcrc/group/earthscience/rjackson/MERRA2/hou_reduced/%sFLUXV*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXV\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 2] = x2\n",
    "    classification = np.array(list(map(get_air_now_label, times)))\n",
    "    where_valid = np.isfinite(classification)\n",
    "    inputs = inputs[where_valid, :, :, :]\n",
    "    classification = classification[where_valid]\n",
    "    # Use classification from x days ahead\n",
    "    if lag > 0:\n",
    "        classification = classification[lag*8:]\n",
    "        inputs = inputs[:lag*8, :, : :]\n",
    "\n",
    "    y = tf.one_hot(classification, 5).numpy()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "            inputs, y, test_size=0.20)\n",
    "    shape = inputs.shape\n",
    "    x_dataset_train = {'input_1': np.squeeze(x_train[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_train[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_train[:, :, :, 2])}\n",
    "    x_dataset_test = {'input_1': np.squeeze(x_test[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_test[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_test[:, :, :, 2])}\n",
    "\n",
    "    return x_dataset_train, x_dataset_test, y_train, y_test, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dea24",
   "metadata": {},
   "source": [
    "Construct the parallel CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(shape, the_dict):\n",
    "    width = shape[2]\n",
    "    height = shape[1]\n",
    "    mpool_1s = []\n",
    "    in_layers = []\n",
    "    for j in range(shape[3]):\n",
    "        inp_layer = Input(shape=(height, width, 1), name=\"input_%d\" % (j + 1))\n",
    "        mpool_1 = inp_layer\n",
    "        in_layers.append(inp_layer)\n",
    "        for i in range(the_dict['num_layers']):\n",
    "            conv2d_1 = Conv2D(the_dict['num_channels'],\n",
    "                (2, 2), activation=the_dict['activation'], padding='same',\n",
    "                kernel_initializer='he_normal')(mpool_1)\n",
    "            conv2d_1 = BatchNormalization()(conv2d_1)\n",
    "            mpool_1 = MaxPooling2D((2, 2))(conv2d_1)\n",
    "        mpool_1s.append(Flatten()(mpool_1))\n",
    "    flat_1 = Add()(mpool_1s)\n",
    "    \n",
    "    for i in range(the_dict['num_dense_layers']):\n",
    "        flat_1 = Dense(the_dict['num_dense_nodes'], activation='relu'\n",
    "                )(flat_1)\n",
    "        flat_1 = BatchNormalization()(flat_1)\n",
    "\n",
    "    output = Dense(5, activation=\"softmax\", name=\"class\")(flat_1)\n",
    "    \n",
    "    return Model(in_layers, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict):\n",
    "    x_ds_train, x_ds_test, y_train, y_test, shape = load_data(config[\"species\"])\n",
    "    model = classifier_model(shape, config)\n",
    "    model.compile(optimizer=Adam(lr=config[\"learning_rate\"]),\n",
    "        loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "    model.summary() \n",
    "    history = model.fit(\n",
    "            x_ds_train, y_train, \n",
    "            validation_data=(x_ds_test, y_test), epochs=config[\"num_epochs\"],\n",
    "            batch_size=config[\"batch_size\"])\n",
    "    model.save('../models/classifier-1day')\n",
    "    return history.history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
