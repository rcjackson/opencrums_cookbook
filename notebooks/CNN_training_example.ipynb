{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5e6435-fc49-4873-8313-0208b4daa4c4",
   "metadata": {},
   "source": [
    "### Code for training Convolutional Neural Network\n",
    "\n",
    "First, load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d997b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjackson/.conda/envs/pydda_env/lib/python3.9/site-packages/xarray/backends/cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Add, ReLU, Conv2DTranspose, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bab506-7b59-4e03-bcb4-34c5e7c16cbe",
   "metadata": {},
   "source": [
    "Next, we load the EPA AirNow data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda44276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "air_now_data = glob.glob('/lcrc/group/earthscience/rjackson/epa_air_now/*.csv')\n",
    "air_now_df = pd.concat(map(pd.read_csv, air_now_data))\n",
    "air_now_df['datetime'] = pd.to_datetime(air_now_df['DateObserved'] + ' 00:00:00')\n",
    "air_now_df = air_now_df.set_index('datetime')\n",
    "air_now_df = air_now_df.sort_index()\n",
    "print(air_now_df['CategoryNumber'].values.min())\n",
    "\n",
    "# How many timesteps ahead should we predict?\n",
    "lag = 0\n",
    "site = \"hou\"\n",
    "\n",
    "# Insert the path to your cropped dataset here\n",
    "input_merra_path = '/lcrc/group/earthscience/rjackson/MERRA2/hou_reduced/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f310c",
   "metadata": {},
   "source": [
    "This next block of code will grab the EPA AirNow classification given a particular datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d87519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_air_now_label(time):\n",
    "    if np.min(np.abs((air_now_df.index - time))) > timedelta(days=1):\n",
    "        return np.nan\n",
    "    ind = np.argmin(np.abs(air_now_df.index - time))\n",
    "    return air_now_df['CategoryNumber'].values[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487cd07-60a8-47ec-ba64-1b41e1185116",
   "metadata": {},
   "source": [
    "This next block of code loads the cropped MERRA2 dataset. \n",
    "\n",
    "load_data takes in a list of aerosol species as inputs. The aerosol species are the abbreviations used in the MERRA2 dataset (i.e. BC, SO4, SO2, DU, SS, OC, etc.). In addition, the *test_data_size* parameter specifies the fraction of the dataset to be set aside as a testing dataset for training.\n",
    "\n",
    "The returned parameters from load_data are:\n",
    "\n",
    "   * x_dataset_train, x_dataset_test: Dictionaries containing the test and training dataset inputs\n",
    "   * y_train, y_test: The training and test labels corresponding to x_dataset\n",
    "   * shape: The shape of the input x data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f060af-adf4-4dbe-b3c4-e83e998ffdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(species, test_data_size=0.20):\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sCMASS*.nc' % species).sortby('time')\n",
    "    print(ds)\n",
    "    times = np.array(list(map(pd.to_datetime, ds.time.values)))\n",
    "    x = ds[\"%sCMASS\" % species].values\n",
    "    old_shape = x.shape\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = scaler.transform(\n",
    "            np.reshape(x, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x = np.reshape(x, old_shape)\n",
    "    inputs = np.zeros((old_shape[0], old_shape[1], old_shape[2], 3))\n",
    "    inputs[:, :, :, 0] = x\n",
    "    ds.close()\n",
    "    if species == \"SO4\" or species == \"DMS\" or species == \"SO2\":\n",
    "       inp = \"SU\"\n",
    "    else:\n",
    "       inp = species\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sFLUXU*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXU\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 1] = x2\n",
    "    ds.close()\n",
    "    ds = xr.open_mfdataset(input_merra_path + '%sFLUXV*.nc' % inp).sortby('time')\n",
    "    x2 = ds[\"%sFLUXV\" % inp].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = scaler.transform(\n",
    "            np.reshape(x2, (old_shape[0], old_shape[1] * old_shape[2])))\n",
    "    x2 = np.reshape(x2, old_shape)\n",
    "    inputs[:, :, :, 2] = x2\n",
    "    classification = np.array(list(map(get_air_now_label, times)))\n",
    "    where_valid = np.isfinite(classification)\n",
    "    inputs = inputs[where_valid, :, :, :]\n",
    "    classification = classification[where_valid]\n",
    "    # Use classification from x days ahead\n",
    "    if lag > 0:\n",
    "        classification = classification[lag*8:]\n",
    "        inputs = inputs[:lag*8, :, : :]\n",
    "\n",
    "    y = tf.one_hot(classification, 5).numpy()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "            inputs, y, test_size=test_data_size)\n",
    "    shape = inputs.shape\n",
    "    x_dataset_train = {'input_1': np.squeeze(x_train[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_train[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_train[:, :, :, 2])}\n",
    "    x_dataset_test = {'input_1': np.squeeze(x_test[:, :, :, 0]),\n",
    "            'input_2': np.squeeze(x_test[:, :, :, 1]),\n",
    "            'input_3': np.squeeze(x_test[:, :, :, 2])}\n",
    "\n",
    "    return x_dataset_train, x_dataset_test, y_train, y_test, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dea24",
   "metadata": {},
   "source": [
    "Construct the parallel CNN model. This procedure takes in 3 parameters:\n",
    "\n",
    "   * shape: the shape of the input data as a tuple\n",
    "   * the_dict: A dictionary containing the model hyperparameters\n",
    "   * dataset: The input dataset returned by *load_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55edbb74-3ef0-4cb4-8c0d-6d718e91a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(shape, the_dict, dataset):\n",
    "    width = shape[2]\n",
    "    height = shape[1]\n",
    "    mpool_1s = []\n",
    "    in_layers = []\n",
    "    dict_keys = list(dataset.keys())\n",
    "    for j in range(len(dict_keys)):\n",
    "        inp_layer = Input(shape=(height, width, 1), name=dict_keys[j])\n",
    "        mpool_1 = inp_layer\n",
    "        in_layers.append(inp_layer)\n",
    "        for i in range(the_dict['num_layers']):\n",
    "            conv2d_1 = Conv2D(the_dict['num_channels'],\n",
    "                (2, 2), activation=the_dict['activation'], padding='same',\n",
    "                kernel_initializer='he_normal')(mpool_1)\n",
    "            conv2d_1 = BatchNormalization()(conv2d_1)\n",
    "            mpool_1 = MaxPooling2D((2, 2))(conv2d_1)\n",
    "        mpool_1s.append(Flatten()(mpool_1))\n",
    "    flat_1 = Add()(mpool_1s)\n",
    "    \n",
    "    for i in range(the_dict['num_dense_layers']):\n",
    "        flat_1 = Dense(the_dict['num_dense_nodes'], activation='relu'\n",
    "                )(flat_1)\n",
    "        flat_1 = BatchNormalization()(flat_1)\n",
    "\n",
    "    output = Dense(5, activation=\"softmax\", name=\"class\")(flat_1)\n",
    "    \n",
    "    return Model(in_layers, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fae491-3fd3-47f5-8202-caf43ead4ade",
   "metadata": {},
   "source": [
    "Do the model training and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict):\n",
    "    x_ds_train = {}\n",
    "    x_ds_test = {}\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    # Adjust to the species you want to have\n",
    "    species_list = config['species_list']\n",
    "    \n",
    "    for species in species_list:\n",
    "        print(species)\n",
    "        x_ds_train1, x_ds_test1, y_train, y_test, shape = load_data(species)\n",
    "        x_ds_train.update(x_ds_train1)\n",
    "        x_ds_test.update(x_ds_test1)\n",
    "    \n",
    "    model = classifier_model(shape, config, x_ds_test)\n",
    "    model.compile(optimizer=Adam(lr=config[\"learning_rate\"]),\n",
    "        loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    # AQI classes inbalanced, need weights\n",
    "    class_weight = {0: 1, 1: 2, 2: 20, 3: 90, 4: 510}\n",
    "    history = model.fit(\n",
    "            x_ds_train, y_train, \n",
    "            validation_data=(x_ds_test, y_test), epochs=config[\"num_epochs\"],\n",
    "            batch_size=config[\"batch_size\"], class_weight=class_weight)\n",
    "    model.save('classifier-%s-lag-%d' % (site, lag))\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ee918-a833-4781-ba40-392f5b5caf48",
   "metadata": {},
   "source": [
    "Call the model training code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915325d9-784d-49a0-851c-93fe664d553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjackson/.conda/envs/pydda_env/lib/python3.9/site-packages/xarray/backends/plugins.py:61: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon        (lon) float64 -105.0 -104.4 -103.8 ... -86.88 -86.25 -85.62\n",
      "  * lat        (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 33.0 33.5 34.0 34.5\n",
      "  * time       (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    SSCMASS    (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "    SSCMASS25  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 13:52:27.969958: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-21 13:52:28.469454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38396 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO4\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon       (lon) float64 -105.0 -104.4 -103.8 -103.1 ... -86.88 -86.25 -85.62\n",
      "  * lat       (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 33.0 33.5 34.0 34.5\n",
      "  * time      (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    SO4CMASS  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "SO2\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon       (lon) float64 -105.0 -104.4 -103.8 -103.1 ... -86.88 -86.25 -85.62\n",
      "  * lat       (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 33.0 33.5 34.0 34.5\n",
      "  * time      (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    SO2CMASS  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "OC\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 -105.0 -104.4 -103.8 -103.1 ... -86.88 -86.25 -85.62\n",
      "  * lat      (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 32.5 33.0 33.5 34.0 34.5\n",
      "  * time     (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    OCCMASS  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "DU\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon        (lon) float64 -105.0 -104.4 -103.8 ... -86.88 -86.25 -85.62\n",
      "  * lat        (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 33.0 33.5 34.0 34.5\n",
      "  * time       (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    DUCMASS    (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "    DUCMASS25  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "DMS\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon       (lon) float64 -105.0 -104.4 -103.8 -103.1 ... -86.88 -86.25 -85.62\n",
      "  * lat       (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 33.0 33.5 34.0 34.5\n",
      "  * time      (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    DMSCMASS  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n",
      "BC\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (lon: 32, lat: 20, time: 90744)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 -105.0 -104.4 -103.8 -103.1 ... -86.88 -86.25 -85.62\n",
      "  * lat      (lat) float64 25.0 25.5 26.0 26.5 27.0 ... 32.5 33.0 33.5 34.0 34.5\n",
      "  * time     (time) datetime64[ns] 2010-01-01T00:30:00 ... 2020-05-08T23:30:00\n",
      "Data variables:\n",
      "    BCCMASS  (time, lat, lon) float32 dask.array<chunksize=(8760, 20, 32), meta=np.ndarray>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjackson/.conda/envs/pydda_env/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 20, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 20, 32, 128)  640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 20, 32, 128)  640         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 20, 32, 128)  640         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 20, 32, 128)  512        ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 20, 32, 128)  512        ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 20, 32, 128)  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 10, 16, 128)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 10, 16, 128)  0          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 10, 16, 128)  0          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 10, 16, 128)  65664       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 10, 16, 128)  65664       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 10, 16, 128)  65664       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 10, 16, 128)  512        ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 10, 16, 128)  512        ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 10, 16, 128)  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 5, 8, 128)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 5, 8, 128)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 5, 8, 128)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5120)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 5120)         0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 5120)         0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 5120)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 8)            40968       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8)           32          ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 8)            72          ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8)           32          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            72          ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 8)           32          ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " class (Dense)                  (None, 5)            45          ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 243,237\n",
      "Trainable params: 241,653\n",
      "Non-trainable params: 1,584\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 13:56:35.550070: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n",
      "2022-10-21 13:56:39.240906: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-21 13:56:43.187227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5171/5171 [==============================] - 37s 5ms/step - loss: 17.7415 - acc: 0.1739 - val_loss: 1.6914 - val_acc: 0.2393\n",
      "Epoch 2/10\n",
      "5171/5171 [==============================] - 23s 5ms/step - loss: 13.6297 - acc: 0.2408 - val_loss: 1.7013 - val_acc: 0.2383\n",
      "Epoch 3/10\n",
      "5171/5171 [==============================] - 23s 4ms/step - loss: 11.5479 - acc: 0.2662 - val_loss: 1.5164 - val_acc: 0.2916\n",
      "Epoch 4/10\n",
      "5171/5171 [==============================] - 23s 5ms/step - loss: 10.2620 - acc: 0.2709 - val_loss: 1.5523 - val_acc: 0.2610\n",
      "Epoch 5/10\n",
      "5171/5171 [==============================] - 23s 5ms/step - loss: 9.2578 - acc: 0.2775 - val_loss: 1.5187 - val_acc: 0.2725\n",
      "Epoch 6/10\n",
      "5171/5171 [==============================] - 23s 4ms/step - loss: 8.3513 - acc: 0.2832 - val_loss: 1.6070 - val_acc: 0.2544\n",
      "Epoch 7/10\n",
      "5171/5171 [==============================] - 23s 4ms/step - loss: 7.5896 - acc: 0.2909 - val_loss: 1.4485 - val_acc: 0.2831\n",
      "Epoch 8/10\n",
      "5171/5171 [==============================] - 23s 4ms/step - loss: 7.3933 - acc: 0.2979 - val_loss: 1.4658 - val_acc: 0.2802\n",
      "Epoch 9/10\n",
      "5171/5171 [==============================] - 23s 5ms/step - loss: 6.8175 - acc: 0.2969 - val_loss: 1.3488 - val_acc: 0.2961\n",
      "Epoch 10/10\n",
      "3696/5171 [====================>.........] - ETA: 5s - loss: 6.5437 - acc: 0.3028"
     ]
    }
   ],
   "source": [
    "default_config = {\n",
    "        \"num_epochs\": 10,        # Number of training epochs. Recommended to use 200+ fo, 10 here is to make cookbook short\n",
    "        \"num_channels\": 128,     # Number of channels in Conv2D layer \n",
    "        \"learning_rate\": 0.0001, # Learning rate for training\n",
    "        \"num_dense_nodes\": 8,    # Number of nodes in each dense layer of the classifier\n",
    "        \"num_dense_layers\": 3,   # Number of layers in classifier portion\n",
    "        \"activation\": \"relu\",    # Activation function used (ReLU is typically good)\n",
    "        \"batch_size\": 10,        # Batch size for training\n",
    "        \"num_layers\": 2,         # Number of Conv2D layers in feature extractor\n",
    "        # Aerosol species to use in model training\n",
    "        \"species_list\": ['SS', 'SO4', 'SO2', 'OC', 'DU', 'DMS', 'BC']}\n",
    "\n",
    "history = run(default_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
